{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Image Captioner with CLIP and LM\n",
    "\n",
    "[CLIP](https://github.com/openai/CLIP)（Contrastive Language–Image Pretraining） 是 OpenAI 提出的多模态预训练模型，结合了图像和文本两种模态，以实现强大的跨模态理解能力。\n",
    "\n",
    "语言模型（Language Model, 简称 LM）是自然语言处理（NLP）中的核心工具，其目标是理解和生成符合语言规律的文本。它以概率的形式建模语言序列，预测文本中单词或标记的出现概率。\n",
    "\n",
    "其中[GPT-2](https://huggingface.co/openai-community/gpt2)（Generative Pre-trained Transformer 2）是 OpenAI 在 2019 年发布的一种语言生成模型，是 GPT（Generative Pre-trained Transformer）系列的第二代模型，展示了在文本生成、总结、翻译等任务上的强大能力。\n",
    "\n",
    "\n",
    "## (1)动机：\n",
    "在Image Caption任务中能否结合**CLIP**的跨模态理解能力和语言模型的文本生成能力，从实现准确的图像理解和语言表达呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)环境依赖\n",
    "\n",
    "请参考`clip-captioner/requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3)数据准备\n",
    "Download [train_captions](https://drive.google.com/file/d/1D3EzUK1d1lNhD2hAvRiKPThidiVbP2K_/view?usp=sharing) to `clip-captioner/data/coco/annotations`.\n",
    "\n",
    "Download [training images](http://images.cocodataset.org/zips/train2014.zip) and [validation images](http://images.cocodataset.org/zips/val2014.zip) and unzip (We use Karpathy et el. split).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)Dataset补全\n",
    "- 请你补全`clip-captioner/data/dataset.py`中的ImageCaptionDataset（line20）和dataloader的collate function（line26）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    # TODO: 需要实现一个 ImageCaptionDataset\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def cl_fn(batch, tokenizer):\n",
    "    # TODO: 需要实现一个 collate function\n",
    "    pass\n",
    "\n",
    "    # return img_emb, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据你实现的dataset补全`clip-captioner/evaluate.py`（line109）和`clip-captioner/training.py`(line50)中的ImageCaptionDataset的具体实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 需要你自己实现一个ImageCaptionDataset在`data/dataset.py`中\n",
    "dataset = ImageCaptionDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5)model补全\n",
    "请你实现一个TransformerEncoder，要求多层（num_layers），多头（n_heads），禁止： 直接使用 nn.TransformerEncoder、nn.TransformerEncoderLayer、nn.MultiheadAttention\n",
    "\n",
    "具体参见`clip-captioner/model/model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  请你实现一个TransformerEncoder，要求多层（num_layers），多头（n_heads）\n",
    "# 禁止： 直接使用 nn.TransformerEncoder、nn.TransformerEncoderLayer、nn.MultiheadAttention\n",
    "\n",
    "# self.transformer_encoder = nn.TransformerEncoder(\n",
    "#     nn.TransformerEncoderLayer(\n",
    "#         d_model=embed_size,\n",
    "#         nhead=n_heads,\n",
    "#         dim_feedforward=embed_size * forward_expansion,\n",
    "#         dropout=dropout,\n",
    "#         batch_first=True,\n",
    "#         device=device,\n",
    "#     ),\n",
    "#     num_layers=num_layers,\n",
    "# ).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6)开始训练？\n",
    "\n",
    "- 在训练前还需要你补全一个training loop,参见`clip-captioner/model/trainer.py`(line71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (img_emb, cap, att_mask) in enumerate(loop):\n",
    "    # TODO: 请你实现一个 training loop\n",
    "    pass\n",
    "\n",
    "    loop.set_description(\n",
    "        f\"Epoch: {self.epoch} | Loss: {total_loss / (batch_idx + 1):.3f}\"\n",
    "    )\n",
    "    loop.refresh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练的launcher主要是`clip-captioner/training.py`,需要两个输入参数，分别表示你要保存的Checkpoint name和模型大小（两种： S和L,配置区别参见`clip-captioner/utils/config.py`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\n",
    "    \"-C\", \"--checkpoint-name\", type=str, default=\"\", help=\"Checkpoint name\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"-S\",\n",
    "    \"--size\",\n",
    "    type=str,\n",
    "    default=\"S\",\n",
    "    help=\"Model size [S, L]\",\n",
    "    choices=[\"S\", \"L\", \"s\", \"l\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7)结果可视化\n",
    "请运行`clip-captioner/evaluate.py`，并保存**5个可视化结果**（图片+生成的描述）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## (8)还能改进？（加分题）\n",
    "当下各种LLM已经展示了比gpt2更强的文本生成能力。\n",
    "你能否把gpt2替换成一个更强的LLM（例如https://hf-mirror.com/Qwen/Qwen2.5-0.5B），提升Image Caption模型的能力？\n",
    "\n",
    "\n",
    "**改进描述**\n",
    "请在下面描述你具体的改进，在哪个文件实现了什么功能,以及改进后结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
