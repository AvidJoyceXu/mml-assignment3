{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3-4: Image Captioning with CLIP and LM\n",
    "\n",
    "**In this 3-4 task, there are a total of 9 steps, with a total score of 40 points.**\n",
    "\n",
    "[CLIP](https://github.com/openai/CLIP) (Contrastive Language–Image Pretraining) is a multimodal pretraining model proposed by OpenAI, combining image and text modalities to achieve robust cross-modal understanding capabilities.\n",
    "\n",
    "Language models (LM) are core tools in natural language processing (NLP), aiming to understand and generate text consistent with linguistic rules. They model language sequences in a probabilistic form, predicting the likelihood of words or tokens in a text.\n",
    "\n",
    "Among them, [GPT-2](https://huggingface.co/openai-community/gpt2) (Generative Pre-trained Transformer 2) is a language generation model released by OpenAI in 2019. It is the second generation in the GPT (Generative Pre-trained Transformer) series and demonstrates strong capabilities in tasks such as text generation, summarization, and translation.\n",
    "\n",
    "## (1) Motivation:\n",
    "In the 3-4 task, can the cross-modal understanding capability of **CLIP** be combined with the text generation capability of language models to achieve both accurate image understanding and expressive language generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Dependencies\n",
    "\n",
    "Please refer to `clip-captioner/requirements.txt` for the list of required dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Dataset Download\n",
    "Download [train_captions](https://drive.google.com/file/d/1D3EzUK1d1lNhD2hAvRiKPThidiVbP2K_/view?usp=sharing) to `clip-captioner/data/coco/annotations`.\n",
    "\n",
    "Download [training images](http://images.cocodataset.org/zips/train2014.zip) and [validation images](http://images.cocodataset.org/zips/val2014.zip) and unzip (We use Karpathy et el. split).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)Dataset Implementation [10 pts]\n",
    "\n",
    "Please implement the ImageCaptionDataset (line 20) in `clip-captioner/data/dataset.py` and the collate function (line 26) for the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    # TODO: 需要实现一个 ImageCaptionDataset\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def cl_fn(batch, tokenizer):\n",
    "    # TODO: 需要实现一个 collate function\n",
    "    pass\n",
    "\n",
    "    # return img_emb, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Dataset Usage [2pts]\n",
    "Please complete the specific instances of `ImageCaptionDataset` in `clip-captioner/evaluate.py` (line 109) and `clip-captioner/training.py` (line 50) based on your implemented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 需要你自己实现一个ImageCaptionDataset在`data/dataset.py`中\n",
    "dataset = ImageCaptionDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6)Model Implementation [5pts]\n",
    "Please implement a `TransformerEncoder` with multiple layers (`num_layers`) and multi-head attention (`n_heads`), without directly using `nn.TransformerEncoder`, `nn.TransformerEncoderLayer`, or `nn.MultiheadAttention`.\n",
    "\n",
    "Refer to `clip-captioner/model/model.py` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  请你实现一个TransformerEncoder，要求多层（num_layers），多头（n_heads）\n",
    "# 禁止： 直接使用 nn.TransformerEncoder、nn.TransformerEncoderLayer、nn.MultiheadAttention\n",
    "\n",
    "# self.transformer_encoder = nn.TransformerEncoder(\n",
    "#     nn.TransformerEncoderLayer(\n",
    "#         d_model=embed_size,\n",
    "#         nhead=n_heads,\n",
    "#         dim_feedforward=embed_size * forward_expansion,\n",
    "#         dropout=dropout,\n",
    "#         batch_first=True,\n",
    "#         device=device,\n",
    "#     ),\n",
    "#     num_layers=num_layers,\n",
    "# ).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6)Model Train[5pts]\n",
    "\n",
    "You also need to complete a training loop before training. Refer to `clip-captioner/model/trainer.py` (line 71)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (img_emb, cap, att_mask) in enumerate(loop):\n",
    "    # TODO: 请你实现一个 training loop\n",
    "    pass\n",
    "\n",
    "    loop.set_description(\n",
    "        f\"Epoch: {self.epoch} | Loss: {total_loss / (batch_idx + 1):.3f}\"\n",
    "    )\n",
    "    loop.refresh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The training launcher is primarily `clip-captioner/training.py`, which requires two input arguments: the name of the checkpoint to save and the model size (two options: `S` and `L`, with configuration differences detailed in `clip-captioner/utils/config.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\n",
    "    \"-C\", \"--checkpoint-name\", type=str, default=\"\", help=\"Checkpoint name\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"-S\",\n",
    "    \"--size\",\n",
    "    type=str,\n",
    "    default=\"S\",\n",
    "    help=\"Model size [S, L]\",\n",
    "    choices=[\"S\", \"L\", \"s\", \"l\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7)  Visualization [3pts]  \n",
    "Please run `clip-captioner/evaluate.py` and save **5 visualization results** (images + generated captions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## (8) Further Improvements?  [10pts]\n",
    "Modern large language models (LLMs) have demonstrated stronger text generation capabilities compared to GPT-2.  \n",
    "Can you replace GPT-2 with a more powerful LLM (e.g., [Qwen2.5-0.5B](https://hf-mirror.com/Qwen/Qwen2.5-0.5B)) to enhance the performance of the Image Captioning model?  \n",
    "\n",
    "## (9) Improvement Description [5pts]\n",
    "Please describe your specific improvements below, including what functionalities were implemented in which files, and the results after the improvements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO: Improvement Description"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
